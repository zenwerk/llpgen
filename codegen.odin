package llpgen

import "core:fmt"
import "core:slice"
import "core:strings"

// コード生成の入力
Codegen_Input :: struct {
	grammar:  ^Grammar,
	firsts:   ^First_Sets,
	follows:  ^Follow_Sets,
	states:   ^[dynamic]Gen_State,
	op_loops: ^map[string]Operator_Loop,
}

// Grammar から Token 型名を取得 (デフォルト: "Token")
@(private = "file")
get_token_type :: proc(g: ^Grammar) -> string {
	return g.token_type_name if len(g.token_type_name) > 0 else "Token"
}

// Grammar から Token_Type enum 名を取得 (Token 型名 + "_Type")
@(private = "file")
get_token_enum_type :: proc(g: ^Grammar) -> string {
	tk := get_token_type(g)
	return fmt.tprintf("%s_Type", tk)
}

// Grammar から Node 型名を取得 (デフォルト: "Node")
@(private = "file")
get_node_type :: proc(g: ^Grammar) -> string {
	return g.node_type_name if len(g.node_type_name) > 0 else "Node"
}

// Grammar から node_free 関数名を取得 ("<node_type>_free" の形式, 先頭小文字)
@(private = "file")
get_node_free :: proc(g: ^Grammar) -> string {
	node := get_node_type(g)
	if len(node) == 0 { return "node_free" }
	// PascalCase/CamelCase を snake_case に変換
	buf := make([dynamic]u8, 0, len(node) * 2, context.temp_allocator)
	for i := 0; i < len(node); i += 1 {
		ch := node[i]
		if ch >= 'A' && ch <= 'Z' {
			if i > 0 && node[i - 1] != '_' {
				append(&buf, '_')
			}
			append(&buf, ch + 32) // to lowercase
		} else {
			append(&buf, ch)
		}
	}
	return fmt.tprintf("%s_free", string(buf[:]))
}

// コード生成メイン
codegen :: proc(input: Codegen_Input) -> string {
	b: strings.Builder
	strings.builder_init(&b)

	emit_header(&b, input.grammar)
	emit_state_enum(&b, input.states)
	emit_event_enum(&b, input)
	emit_common_types(&b, input.grammar)
	emit_core_functions(&b, input.grammar)
	emit_push_token(&b, input.grammar, input.states)
	emit_parse_start(&b, input.grammar, input.states)
	emit_parse_functions(&b, input)

	return strings.to_string(b)
}

// ========================================================================
// ヘッダ (package宣言 + import)
// ========================================================================

@(private = "file")
emit_header :: proc(b: ^strings.Builder, g: ^Grammar) {
	pkg := g.package_name if len(g.package_name) > 0 else "parser"
	node := get_node_type(g)
	node_free := get_node_free(g)

	fmt.sbprintf(b,
`// Code generated by llpgen. DO NOT EDIT.
package %s

import "core:container/queue"
import "core:fmt"

// ========================================================================
// このパーサーを使用するには、以下の型と関数を別ファイルで定義してください:
//
//   %s :: struct {{ ... }}       // AST ノード型
//   %s(n: ^%s)                  // ノードの再帰的解放
//   on_parse_event(p: ^Parser, event: Parse_Event, tk: ^Token, top: ^Parse_State)
//                                // パースイベントハンドラ
//
// トークン型は _token.odin に自動生成されます。
// ========================================================================

`, pkg, node, node_free, node)
}

// ========================================================================
// 4.1a: Parse_State_Kind enum
// ========================================================================

@(private = "file")
emit_state_enum :: proc(b: ^strings.Builder, states: ^[dynamic]Gen_State) {
	fmt.sbprint(b, "// パーサーの状態\n")
	fmt.sbprint(b, "Parse_State_Kind :: enum {\n")
	fmt.sbprint(b, "\t// 基本状態\n")
	fmt.sbprint(b, "\tStart,\n")
	fmt.sbprint(b, "\tEnd,\n")
	fmt.sbprint(b, "\tError,\n")

	// 規則ごとにグループ分けして出力
	current_rule := ""
	for &s in states {
		if s.rule != current_rule {
			current_rule = s.rule
			fmt.sbprintf(b, "\t// -- %s --\n", s.rule)
		}
		fmt.sbprintf(b, "\t%s,\n", s.name)
	}

	fmt.sbprint(b, "}\n\n")
}

// ========================================================================
// 4.1a-2: Parse_Event enum (パースイベント種別)
// ========================================================================

// イベント名導出: 開始状態での Terminal マッチ
// rule="factor", terminal="Number" → "Factor_Number"
@(private = "file")
event_name_for_match :: proc(rule_name, terminal_name: string) -> string {
	rule_pascal := to_pascal_case(rule_name, context.temp_allocator)
	return fmt.tprintf("%s_%s", rule_pascal, terminal_name)
}

// イベント名導出: 演算子ループの演算子マッチ
// states から Op 状態名を検索して返す (例: "Expr_Op")
@(private = "file")
event_name_for_operator :: proc(rule_name: string, states: ^[dynamic]Gen_State) -> string {
	for &s in states {
		if s.rule == rule_name && s.prod == -1 && s.pos == 1 {
			return s.name
		}
	}
	// フォールバック: 見つからない場合は従来の命名
	rule_pascal := to_pascal_case(rule_name, context.temp_allocator)
	return fmt.tprintf("%s_Op", rule_pascal)
}

@(private = "file")
emit_event_enum :: proc(b: ^strings.Builder, input: Codegen_Input) {
	g := input.grammar
	states := input.states

	// イベント名を収集
	events: [dynamic]string
	event_rules: [dynamic]string // 各イベントが属する規則名 (グループ分け用)
	defer delete(events)
	defer delete(event_rules)

	for &rule in g.rules {
		if input.op_loops != nil && rule.name in input.op_loops^ {
			// 演算子ループ規則
			loop := &input.op_loops[rule.name]

			// カテゴリ D: ベースケースの先頭 Terminal
			for base_idx in loop.base_prods {
				base_prod := &rule.productions[base_idx]
				if len(base_prod.symbols) > 0 && base_prod.symbols[0].kind == .Terminal {
					name := event_name_for_match(rule.name, base_prod.symbols[0].name)
					append(&events, strings.clone(name, context.temp_allocator))
					append(&event_rules, rule.name)
				}
			}

			// カテゴリ C: 演算子マッチ (状態名と一致させる)
			name := event_name_for_operator(rule.name, states)
			append(&events, strings.clone(name, context.temp_allocator))
			append(&event_rules, rule.name)
		} else {
			// 通常規則

			// カテゴリ A: 開始状態での Terminal マッチ
			for &prod in rule.productions {
				if len(prod.symbols) > 0 && prod.symbols[0].kind == .Terminal {
					name := event_name_for_match(rule.name, prod.symbols[0].name)
					append(&events, strings.clone(name, context.temp_allocator))
					append(&event_rules, rule.name)
				}
			}

			// カテゴリ B: 中間状態 (Await_ 状態) — 状態名をそのまま使用
			for &s in states {
				if s.rule == rule.name && s.pos > 0 {
					append(&events, strings.clone(s.name, context.temp_allocator))
					append(&event_rules, rule.name)
				}
			}
		}
	}

	// enum 出力
	fmt.sbprint(b, "// パースイベント種別\n")
	fmt.sbprint(b, "Parse_Event :: enum {\n")
	fmt.sbprint(b, "\tNone,\n")

	current_rule := ""
	for ev, i in events {
		if event_rules[i] != current_rule {
			current_rule = event_rules[i]
			fmt.sbprintf(b, "\t// -- %s --\n", current_rule)
		}
		fmt.sbprintf(b, "\t%s,\n", ev)
	}

	fmt.sbprint(b, "}\n\n")
}

// ========================================================================
// 4.1b: 共通型定義 (Parse_Loop_Action, Parse_Result, Parse_State, Parser)
// ========================================================================

@(private = "file")
emit_common_types :: proc(b: ^strings.Builder, g: ^Grammar) {
	node := get_node_type(g)

	fmt.sbprintf(b,
`// パースループ制御アクション
Parse_Loop_Action :: enum {{
	Break,
	Continue,
}}

// パース結果
Parse_Result :: enum {{
	Parse_End,
	Push_More,
}}

// パーサー状態
Parse_State :: struct {{
	state: Parse_State_Kind,
	node:  ^^%s,    // 現在のノードへのポインタ
	saved: ^%s,     // 保存用ノード
	op:    string,    // 演算子 (必要に応じて)
}}

// パーサー
Parser :: struct {{
	state_stack: queue.Queue(Parse_State),
	root:        ^%s,
	error_msg:   string,
	nerr:        int,
}}

`, node, node, node)
}

// ========================================================================
// 4.1c: パーサーコア関数 (テンプレート)
// ========================================================================

@(private = "file")
emit_core_functions :: proc(b: ^strings.Builder, g: ^Grammar) {
	node := get_node_type(g)
	node_free := get_node_free(g)

	// parser_new
	fmt.sbprint(b,
`// パーサーの初期化
parser_new :: proc() -> ^Parser {
	p := new(Parser)
	queue.init(&p.state_stack, capacity = 16)
	p.root = nil
	p.error_msg = ""
	p.nerr = 0
	parser_begin(p, .Start, &p.root)
	return p
}

`)

	// parser_destroy
	fmt.sbprintf(b,
`// パーサーの破棄
parser_destroy :: proc(p: ^Parser) {{
	if p == nil {{
		return
	}}
	queue.destroy(&p.state_stack)
	if p.root != nil {{
		%s(p.root)
	}}
	free(p)
}}

`, node_free)

	// parser_reset
	fmt.sbprintf(b,
`// パーサーのリセット
parser_reset :: proc(p: ^Parser) {{
	queue.clear(&p.state_stack)
	if p.root != nil {{
		%s(p.root)
	}}
	p.root = nil
	p.error_msg = ""
	p.nerr = 0
	parser_begin(p, .Start, &p.root)
}}

`, node_free)

	// parser_begin
	fmt.sbprintf(b,
`// 新しい状態をスタックにプッシュ
parser_begin :: proc(p: ^Parser, state: Parse_State_Kind, node: ^^%s) {{
	queue.push_front(&p.state_stack, Parse_State{{state = state, node = node}})
}}

`, node)

	// parser_end + parser_get_state
	fmt.sbprint(b,
`// 現在の状態をスタックからポップ
parser_end :: proc(p: ^Parser) {
	if queue.len(p.state_stack) > 0 {
		queue.pop_front(&p.state_stack)
	}
}

// 現在の状態を取得
parser_get_state :: proc(p: ^Parser) -> ^Parse_State {
	if queue.len(p.state_stack) <= 0 {
		return nil
	}
	return queue.front_ptr(&p.state_stack)
}

`)

	// parser_set_state
	fmt.sbprintf(b,
`// 現在の状態を更新
parser_set_state :: proc(p: ^Parser, state: Parse_State_Kind, node: ^^%s = nil) {{
	if queue.len(p.state_stack) <= 0 {{
		return
	}}
	top := parser_get_state(p)
	if top == nil {{
		return
	}}
	top.state = state
	if node != nil {{
		top.node = node
	}}
}}

`, node)

	// parser_error
	fmt.sbprintf(b,
`// エラー状態に遷移
parser_error :: proc(p: ^Parser, msg: string) {{
	p.error_msg = msg
	p.nerr += 1
	if p.root != nil {{
		%s(p.root)
	}}
	p.root = nil
	queue.clear(&p.state_stack)
	parser_begin(p, .Error, &p.root)
}}

`, node_free)

}

// ========================================================================
// 4.1d: parser_push_token ディスパッチ関数
// ========================================================================

@(private = "file")
emit_push_token :: proc(b: ^strings.Builder, g: ^Grammar, states: ^[dynamic]Gen_State) {
	tk_type := get_token_type(g)

	fmt.sbprintf(b,
`// トークンをプッシュしてパース
parser_push_token :: proc(p: ^Parser, token: %s) -> Parse_Result {{
	tk := token
	action: Parse_Loop_Action
	max_iterations := 1000

	for i := 0; i < max_iterations; i += 1 {{
		top := parser_get_state(p)
		if top == nil {{
			break
		}}
		pstate := top.state

		if tk.consumed {{
			break
		}}

		if tk.type == .Error && pstate != .Error {{
			parser_error(p, fmt.tprintf("Lexer error: %%%%s", tk.lexeme))
			break
		}}

		// 状態に応じたパース関数を呼び出す
		#partial switch pstate {{
		case .Start, .End, .Error:
			action = parse_start(p, &tk)
`, tk_type)

	// 各規則の状態に基づくディスパッチを生成
	groups := build_state_groups(g, states)
	defer {
		for &group in groups {
			delete(group.state_names)
		}
		delete(groups)
	}

	for &group in groups {
		fmt.sbprint(b, "\t\tcase ")
		for name, i in group.state_names {
			if i > 0 { fmt.sbprint(b, ", ") }
			fmt.sbprintf(b, ".%s", name)
		}
		fmt.sbprint(b, ":\n")
		fmt.sbprintf(b, "\t\t\taction = parse_%s(p, &tk)\n", group.rule_name)
	}

	fmt.sbprint(b,
`		case:
			fmt.eprintfln("Parse: Unknown state %v", top.state)
			break
		}

		if action == .Break {
			break
		}
	}

	top := parser_get_state(p)
	if top != nil && (top.state == .End || top.state == .Error) {
		return .Parse_End
	}
	return .Push_More
}

`)
}

// 状態グループ (1つの規則に所属する全状態名)
@(private = "file")
State_Group :: struct {
	rule_name:   string,
	state_names: [dynamic]string,
}

@(private = "file")
build_state_groups :: proc(g: ^Grammar, states: ^[dynamic]Gen_State) -> [dynamic]State_Group {
	groups: [dynamic]State_Group
	current_rule := ""

	for &s in states {
		if s.rule != current_rule {
			current_rule = s.rule
			append(&groups, State_Group{
				rule_name = s.rule,
			})
		}
		append(&groups[len(groups) - 1].state_names, s.name)
	}

	return groups
}

// ========================================================================
// 4.1e-0: parse_start 関数
// ========================================================================

@(private = "file")
emit_parse_start :: proc(b: ^strings.Builder, g: ^Grammar, states: ^[dynamic]Gen_State) {
	// 開始規則の状態名を取得
	start_state := ""
	for &s in states {
		if s.rule == g.start_rule && s.pos == 0 {
			start_state = s.name
			break
		}
	}

	tk_type := get_token_type(g)

	fmt.sbprintf(b,
`// 開始状態のパース
parse_start :: proc(p: ^Parser, tk: ^%s) -> Parse_Loop_Action {{
	top := parser_get_state(p)
	if top == nil {{ return .Break }}

	#partial switch top.state {{
	case .Start:
		if tk.type == .Eof {{
			parser_set_state(p, .End)
			return .Break
		}}
		parser_set_state(p, .End)
		parser_begin(p, .%s, top.node)
		return .Continue
	case .End:
		return .Break
	case .Error:
		tk.consumed = true
	}}
	return .Break
}}

`, tk_type, start_state)
}

// ========================================================================
// 4.1e: 各 parse_* 関数のスケルトン生成
// ========================================================================

@(private = "file")
emit_parse_functions :: proc(b: ^strings.Builder, input: Codegen_Input) {
	g := input.grammar

	for &rule in g.rules {
		if input.op_loops != nil && rule.name in input.op_loops^ {
			emit_operator_loop_function(b, input, &rule, &input.op_loops[rule.name])
		} else {
			emit_parse_function(b, input, &rule)
		}
	}
}

@(private = "file")
emit_parse_function :: proc(b: ^strings.Builder, input: Codegen_Input, rule: ^Rule) {
	g := input.grammar
	states := input.states

	// この規則に属する状態を収集
	rule_states: [dynamic]Gen_State
	defer delete(rule_states)
	for &s in states {
		if s.rule == rule.name {
			append(&rule_states, s)
		}
	}

	tk_type := get_token_type(g)

	fmt.sbprintf(b,
`// %s 規則のパース
parse_%s :: proc(p: ^Parser, tk: ^%s) -> Parse_Loop_Action {{
	top := parser_get_state(p)
	if top == nil {{ return .Break }}

	#partial switch top.state {{
`, rule.name, rule.name, tk_type)

	// 開始状態のケース (pos == 0)
	for &s in rule_states {
		if s.pos == 0 {
			emit_rule_start_case(b, input, rule, &s)
		}
	}

	// 中間状態のケース (pos > 0)
	for &s in rule_states {
		if s.pos > 0 {
			emit_intermediate_case(b, input, rule, &s)
		}
	}

	fmt.sbprint(b, "\t}\n")
	fmt.sbprint(b, "\treturn .Break\n")
	fmt.sbprint(b, "}\n\n")
}

// 規則の開始状態ケース: 各 production の FIRST に基づく分岐
@(private = "file")
emit_rule_start_case :: proc(b: ^strings.Builder, input: Codegen_Input, rule: ^Rule, state: ^Gen_State) {
	g := input.grammar

	fmt.sbprintf(b, "\tcase .%s:\n", state.name)

	if len(rule.productions) == 1 {
		// 1つの production のみ: 分岐なし
		emit_production_body(b, input, rule, 0, "\t\t")
	} else {
		// 複数 production: FIRST に基づく分岐
		first_if := true
		for prod_idx := 0; prod_idx < len(rule.productions); prod_idx += 1 {
			prod := &rule.productions[prod_idx]
			if len(prod.symbols) == 0 {
				continue // ε production は後で else として処理
			}

			if first_if {
				fmt.sbprint(b, "\t\t")
				first_if = false
			} else {
				fmt.sbprint(b, " else ")
			}

			// 条件を生成
			emit_production_condition(b, input, prod, rule.name)
			fmt.sbprint(b, " {\n")
			emit_production_body(b, input, rule, prod_idx, "\t\t\t")
			fmt.sbprint(b, "\t\t}")
		}

		// ε production があれば else として出力
		has_epsilon := false
		for prod_idx := 0; prod_idx < len(rule.productions); prod_idx += 1 {
			prod := &rule.productions[prod_idx]
			if len(prod.symbols) == 0 {
				has_epsilon = true
				fmt.sbprint(b, " else {\n")
				fmt.sbprint(b, "\t\t\t// ε production\n")
				fmt.sbprint(b, "\t\t\tparser_end(p)\n")
				fmt.sbprint(b, "\t\t\treturn .Continue\n")
				fmt.sbprint(b, "\t\t}")
				break
			}
		}

		if !has_epsilon && !first_if {
			fmt.sbprint(b, " else {\n")
			fmt.sbprintf(b, "\t\t\tparser_error(p, fmt.tprintf(\"Unexpected token in %s: %%v\", tk.type))\n", rule.name)
			fmt.sbprint(b, "\t\t\treturn .Break\n")
			fmt.sbprint(b, "\t\t}")
		}
		fmt.sbprint(b, "\n")
	}
}

// production の条件式を生成
@(private = "file")
emit_production_condition :: proc(b: ^strings.Builder, input: Codegen_Input, prod: ^Production, rule_name: string = "") {
	g := input.grammar

	if len(prod.symbols) == 0 {
		fmt.sbprint(b, "if true")
		return
	}

	first_sym := prod.symbols[0]
	if first_sym.kind == .Terminal {
		fmt.sbprintf(b, "if tk.type == .%s", first_sym.name)
	} else {
		// Nonterminal: FIRST 集合の全トークンで分岐
		mutable_firsts := input.firsts^
		first_set := compute_first_of_symbols(&mutable_firsts, prod.symbols[:], g)
		defer delete(first_set)

		conditions: [dynamic]string
		defer delete(conditions)
		conditions_set: map[string]bool
		defer delete(conditions_set)
		for tok in first_set {
			if tok == EPSILON_MARKER { continue }
			append(&conditions, tok)
			conditions_set[tok] = true
		}

		// FIRST 集合に ε が含まれる場合、FOLLOW 集合のトークンも追加
		if EPSILON_MARKER in first_set && rule_name != "" && input.follows != nil {
			if rule_name in input.follows^ {
				for tok in input.follows[rule_name] {
					if tok not_in conditions_set {
						append(&conditions, tok)
						conditions_set[tok] = true
					}
				}
			}
		}

		slice.sort(conditions[:])

		if len(conditions) > 0 {
			fmt.sbprint(b, "if ")
			for ci := 0; ci < len(conditions); ci += 1 {
				if ci > 0 { fmt.sbprint(b, " || ") }
				fmt.sbprintf(b, "tk.type == .%s", conditions[ci])
			}
		} else {
			fmt.sbprint(b, "if true /* WARNING: empty FIRST+FOLLOW */")
		}
	}
}

// 1つの production のボディ(開始処理)を生成
@(private = "file")
emit_production_body :: proc(b: ^strings.Builder, input: Codegen_Input, rule: ^Rule, prod_idx: int, indent: string) {
	prod := &rule.productions[prod_idx]

	if len(prod.symbols) == 0 {
		fmt.sbprintf(b, "%s// ε production\n", indent)
		fmt.sbprintf(b, "%sparser_end(p)\n", indent)
		fmt.sbprintf(b, "%sreturn .Continue\n", indent)
		return
	}

	first_sym := prod.symbols[0]

	if first_sym.kind == .Terminal {
		ev := event_name_for_match(rule.name, first_sym.name)
		fmt.sbprintf(b, "%son_parse_event(p, .%s, tk, top)\n", indent, ev)
		fmt.sbprintf(b, "%stk.consumed = true\n", indent)
		if len(prod.symbols) == 1 {
			// 単一 Terminal → parse_end
			fmt.sbprintf(b, "%sparser_end(p)\n", indent)
			fmt.sbprintf(b, "%sreturn .Continue\n", indent)
		} else {
			// Terminal 消費後: 次のシンボル列を処理
			emit_transition_after_pos(b, input, rule, prod_idx, 0, indent)
		}
	} else {
		// Nonterminal → parser_begin
		nonterminal_start := find_state_for(input.states, first_sym.name, 0, 0)
		if len(prod.symbols) == 1 {
			// 単一 Nonterminal → parse_end + begin
			fmt.sbprintf(b, "%sparser_end(p)\n", indent)
			fmt.sbprintf(b, "%sparser_begin(p, .%s, top.node)\n", indent, nonterminal_start)
			fmt.sbprintf(b, "%sreturn .Continue\n", indent)
		} else {
			// Nonterminal の後: 次の実在する状態に set_state + begin
			next_real := find_next_real_state(input.states, rule.name, prod_idx, 1, prod)
			if next_real == "" {
				// 残りの状態なし: 後続の Nonterminal を逆順にスタックに積む
				fmt.sbprintf(b, "%sparser_end(p)\n", indent)
				// 後続の Nonterminal を逆順に begin (スタックなので逆順 = 正順実行)
				for i := len(prod.symbols) - 1; i >= 1; i -= 1 {
					sym := prod.symbols[i]
					if sym.kind == .Nonterminal {
						nt_start := find_state_for(input.states, sym.name, 0, 0)
						fmt.sbprintf(b, "%sparser_begin(p, .%s, top.node)\n", indent, nt_start)
					}
				}
				fmt.sbprintf(b, "%sparser_begin(p, .%s, top.node)\n", indent, nonterminal_start)
			} else {
				fmt.sbprintf(b, "%sparser_set_state(p, .%s)\n", indent, next_real)
				fmt.sbprintf(b, "%sparser_begin(p, .%s, top.node)\n", indent, nonterminal_start)
			}
			fmt.sbprintf(b, "%sreturn .Continue\n", indent)
		}
	}
}

// pos の位置のシンボルを処理した後の遷移を生成
// Terminal 消費後に次が Nonterminal の場合、直接 begin + set_state を行う
@(private = "file")
emit_transition_after_pos :: proc(b: ^strings.Builder, input: Codegen_Input, rule: ^Rule, prod_idx: int, pos: int, indent: string) {
	prod := &rule.productions[prod_idx]
	next_pos := pos + 1

	if next_pos >= len(prod.symbols) {
		// 最後のシンボル → parse_end
		fmt.sbprintf(b, "%sparser_end(p)\n", indent)
		fmt.sbprintf(b, "%sreturn .Continue\n", indent)
		return
	}

	next_sym := prod.symbols[next_pos]

	if next_sym.kind == .Terminal {
		// 次が Terminal → 次の状態に遷移
		next_real := find_next_real_state(input.states, rule.name, prod_idx, next_pos, prod)
		if next_real != "" {
			fmt.sbprintf(b, "%sparser_set_state(p, .%s)\n", indent, next_real)
		}
		fmt.sbprintf(b, "%sreturn .Continue\n", indent)
	} else {
		// 次が Nonterminal → 直接 begin し、その先の状態に set_state
		nonterminal_start := find_state_for(input.states, next_sym.name, 0, 0)
		next_real := find_next_real_state(input.states, rule.name, prod_idx, next_pos + 1, prod)
		if next_real == "" {
			// Nonterminal の後に状態なし: 後続の Nonterminal を逆順にスタックに積む
			fmt.sbprintf(b, "%sparser_end(p)\n", indent)
			for i := len(prod.symbols) - 1; i >= next_pos + 1; i -= 1 {
				sym := prod.symbols[i]
				if sym.kind == .Nonterminal {
					nt_start := find_state_for(input.states, sym.name, 0, 0)
					fmt.sbprintf(b, "%sparser_begin(p, .%s, top.node)\n", indent, nt_start)
				}
			}
			fmt.sbprintf(b, "%sparser_begin(p, .%s, top.node)\n", indent, nonterminal_start)
		} else {
			fmt.sbprintf(b, "%sparser_set_state(p, .%s)\n", indent, next_real)
			fmt.sbprintf(b, "%sparser_begin(p, .%s, top.node)\n", indent, nonterminal_start)
		}
		fmt.sbprintf(b, "%sreturn .Continue\n", indent)
	}
}

// 中間状態のケース
// Phase 4: この状態は常にTerminal位置 (Nonterminal位置は通過状態として生成されない)
@(private = "file")
emit_intermediate_case :: proc(b: ^strings.Builder, input: Codegen_Input, rule: ^Rule, state: ^Gen_State) {
	prod := &rule.productions[state.prod]

	fmt.sbprintf(b, "\tcase .%s:\n", state.name)

	sym := prod.symbols[state.pos]

	// Phase 4: 中間状態は常に Terminal 位置 (Nonterminal 位置は通過状態としてスキップ済み)
	fmt.sbprintf(b, "\t\tif consumed(tk, .%s) {{\n", sym.name)
	fmt.sbprintf(b, "\t\t\ton_parse_event(p, .%s, tk, top)\n", state.name)
	emit_transition_after_pos(b, input, rule, state.prod, state.pos, "\t\t\t")
	fmt.sbprint(b, "\t\t} else {\n")
	fmt.sbprintf(b, "\t\t\tparser_error(p, fmt.tprintf(\"Expected %s, got %%v\", tk.type))\n", sym.name)
	fmt.sbprint(b, "\t\t}\n")
}

// 指定された規則・production・位置に対応する状態名を検索
@(private = "file")
find_state_for :: proc(states: ^[dynamic]Gen_State, rule_name: string, prod_idx: int, pos: int) -> string {
	for &s in states {
		if s.rule == rule_name && s.prod == prod_idx && s.pos == pos {
			return s.name
		}
	}
	// 見つからない場合は開始状態を返す
	for &s in states {
		if s.rule == rule_name && s.pos == 0 {
			return s.name
		}
	}
	return "Error"
}

// 指定された位置以降で最初に存在する状態を検索（通過状態スキップ対応）
// 通過状態（Nonterminal 位置）が生成されていない場合、その先の状態を返す
// 状態が見つからない場合は "" を返す (= parse_end が必要)
@(private = "file")
find_next_real_state :: proc(states: ^[dynamic]Gen_State, rule_name: string, prod_idx: int, from_pos: int, prod: ^Production) -> string {
	for pos := from_pos; pos < len(prod.symbols); pos += 1 {
		for &s in states {
			if s.rule == rule_name && s.prod == prod_idx && s.pos == pos {
				return s.name
			}
		}
	}
	return "" // 残りの状態なし → parse_end が必要
}

// ========================================================================
// 演算子ループ規則のコード生成
// ========================================================================

// 演算子ループのベースケースで Terminal 消費後の残りシンボルを処理
// op_state に遷移した上で、残りの Nonterminal をスタックに積む
@(private = "file")
emit_operator_loop_remaining_symbols :: proc(b: ^strings.Builder, input: Codegen_Input, prod: ^Production, from_pos: int, op_state: string, indent: string) {
	states := input.states

	if from_pos >= len(prod.symbols) {
		// 残りシンボルなし → op_state に遷移
		fmt.sbprintf(b, "%sparser_set_state(p, .%s)\n", indent, op_state)
		fmt.sbprintf(b, "%sreturn .Continue\n", indent)
		return
	}

	// 残りの Nonterminal シンボルをスタックに積む (逆順)
	fmt.sbprintf(b, "%sparser_set_state(p, .%s)\n", indent, op_state)
	for i := len(prod.symbols) - 1; i >= from_pos; i -= 1 {
		sym := prod.symbols[i]
		if sym.kind == .Nonterminal {
			nt_start := find_state_for(states, sym.name, 0, 0)
			fmt.sbprintf(b, "%sparser_begin(p, .%s, top.node)\n", indent, nt_start)
		}
	}
	fmt.sbprintf(b, "%sreturn .Continue\n", indent)
}

// 演算子ループ規則の parse 関数を生成
// 手書きの Expr → Expr_Op パターンに相当するコードを出力
@(private = "file")
emit_operator_loop_function :: proc(b: ^strings.Builder, input: Codegen_Input, rule: ^Rule, loop: ^Operator_Loop) {
	g := input.grammar
	states := input.states
	tk_type := get_token_type(g)

	// この規則に属する状態を収集
	rule_states: [dynamic]Gen_State
	defer delete(rule_states)
	for &s in states {
		if s.rule == rule.name {
			append(&rule_states, s)
		}
	}

	// 開始状態と Op 状態の名前を取得
	start_state := ""
	op_state := ""
	for &s in rule_states {
		if s.pos == 0 { start_state = s.name }
		if s.prod == -1 && s.pos == 1 { op_state = s.name }
	}

	// ベースケースの非終端記号の開始状態名を取得
	base_start := find_state_for(states, loop.base_name, 0, 0)

	fmt.sbprintf(b,
`// %s 規則のパース (演算子ループ: %s (op %s)*)
parse_%s :: proc(p: ^Parser, tk: ^%s) -> Parse_Loop_Action {{
	top := parser_get_state(p)
	if top == nil {{ return .Break }}

	#partial switch top.state {{
	case .%s:
`, rule.name, loop.base_name, rule.name, rule.name, tk_type, start_state)

	// ベースケースの処理
	// ベースケースが単一の非終端記号のみの場合はシンプルに
	if len(loop.base_prods) == 1 {
		base_prod := &rule.productions[loop.base_prods[0]]
		if len(base_prod.symbols) == 1 && base_prod.symbols[0].kind == .Nonterminal {
			// 単純なケース: A : ... | B ; → B をパースして A_Op に遷移
			fmt.sbprintf(b, "\t\tparser_set_state(p, .%s)\n", op_state)
			fmt.sbprintf(b, "\t\tparser_begin(p, .%s, top.node)\n", base_start)
			fmt.sbprint(b, "\t\treturn .Continue\n")
		} else {
			// ベースケースが複数シンボルの場合: 開始処理を通常と同様に生成
			emit_operator_loop_base_case(b, input, rule, loop, op_state)
		}
	} else {
		// 複数のベースケース: 条件分岐
		emit_operator_loop_base_case(b, input, rule, loop, op_state)
	}

	fmt.sbprintf(b, "\tcase .%s:\n", op_state)

	// 演算子チェック: いずれかの演算子にマッチすればベースをパース (ループ)
	fmt.sbprint(b, "\t\tif ")
	for op, i in loop.operators {
		if i > 0 { fmt.sbprint(b, " || ") }
		fmt.sbprintf(b, "tk.type == .%s", op)
	}
	fmt.sbprint(b, " {\n")
	fmt.sbprintf(b, "\t\t\ton_parse_event(p, .%s, tk, top)\n", op_state)
	fmt.sbprint(b, "\t\t\ttk.consumed = true\n")
	fmt.sbprintf(b, "\t\t\tparser_begin(p, .%s, top.node)\n", base_start)
	fmt.sbprint(b, "\t\t\treturn .Continue\n")
	fmt.sbprint(b, "\t\t} else {\n")
	fmt.sbprint(b, "\t\t\tparser_end(p)\n")
	fmt.sbprint(b, "\t\t\treturn .Continue\n")
	fmt.sbprint(b, "\t\t}\n")

	fmt.sbprint(b, "\t}\n")
	fmt.sbprint(b, "\treturn .Break\n")
	fmt.sbprint(b, "}\n\n")
}

// 演算子ループのベースケース生成
@(private = "file")
emit_operator_loop_base_case :: proc(b: ^strings.Builder, input: Codegen_Input, rule: ^Rule, loop: ^Operator_Loop, op_state: string) {
	g := input.grammar
	states := input.states

	if len(loop.base_prods) == 1 {
		base_prod := &rule.productions[loop.base_prods[0]]
		if len(base_prod.symbols) == 0 {
			// ε production
			fmt.sbprintf(b, "\t\tparser_set_state(p, .%s)\n", op_state)
			fmt.sbprint(b, "\t\treturn .Continue\n")
		} else {
			first_sym := base_prod.symbols[0]
			if first_sym.kind == .Terminal {
				ev := event_name_for_match(rule.name, first_sym.name)
				fmt.sbprintf(b, "\t\ton_parse_event(p, .%s, tk, top)\n", ev)
				fmt.sbprintf(b, "\t\ttk.consumed = true\n")
				// 残りのシンボルがある場合は遷移を生成
				emit_operator_loop_remaining_symbols(b, input, base_prod, 1, op_state, "\t\t")
			} else {
				nonterminal_start := find_state_for(states, first_sym.name, 0, 0)
				if len(base_prod.symbols) == 1 {
					fmt.sbprintf(b, "\t\tparser_set_state(p, .%s)\n", op_state)
					fmt.sbprintf(b, "\t\tparser_begin(p, .%s, top.node)\n", nonterminal_start)
					fmt.sbprint(b, "\t\treturn .Continue\n")
				} else {
					// Nonterminal の後に残りシンボルがある場合
					fmt.sbprintf(b, "\t\tparser_set_state(p, .%s)\n", op_state)
					// 残りの Nonterminal を逆順にスタックに積む
					for i := len(base_prod.symbols) - 1; i >= 1; i -= 1 {
						sym := base_prod.symbols[i]
						if sym.kind == .Nonterminal {
							nt_start := find_state_for(states, sym.name, 0, 0)
							fmt.sbprintf(b, "\t\tparser_begin(p, .%s, top.node)\n", nt_start)
						}
					}
					fmt.sbprintf(b, "\t\tparser_begin(p, .%s, top.node)\n", nonterminal_start)
					fmt.sbprint(b, "\t\treturn .Continue\n")
				}
			}
		}
	} else {
		// 複数のベースケース: FIRST に基づく分岐
		first_if := true
		for base_idx in loop.base_prods {
			base_prod := &rule.productions[base_idx]
			if len(base_prod.symbols) == 0 {
				continue // ε は後で処理
			}

			if first_if {
				fmt.sbprint(b, "\t\t")
				first_if = false
			} else {
				fmt.sbprint(b, " else ")
			}

			emit_production_condition(b, input, base_prod, rule.name)
			fmt.sbprint(b, " {\n")

			first_sym := base_prod.symbols[0]
			if first_sym.kind == .Terminal {
				ev := event_name_for_match(rule.name, first_sym.name)
				fmt.sbprintf(b, "\t\t\ton_parse_event(p, .%s, tk, top)\n", ev)
				fmt.sbprint(b, "\t\t\ttk.consumed = true\n")
				// 残りのシンボルがある場合は遷移を生成
				emit_operator_loop_remaining_symbols(b, input, base_prod, 1, op_state, "\t\t\t")
			} else {
				nonterminal_start := find_state_for(states, first_sym.name, 0, 0)
				if len(base_prod.symbols) == 1 {
					fmt.sbprintf(b, "\t\t\tparser_set_state(p, .%s)\n", op_state)
					fmt.sbprintf(b, "\t\t\tparser_begin(p, .%s, top.node)\n", nonterminal_start)
					fmt.sbprint(b, "\t\t\treturn .Continue\n")
				} else {
					// Nonterminal の後に残りシンボルがある場合
					fmt.sbprintf(b, "\t\t\tparser_set_state(p, .%s)\n", op_state)
					for i := len(base_prod.symbols) - 1; i >= 1; i -= 1 {
						sym := base_prod.symbols[i]
						if sym.kind == .Nonterminal {
							nt_start := find_state_for(states, sym.name, 0, 0)
							fmt.sbprintf(b, "\t\t\tparser_begin(p, .%s, top.node)\n", nt_start)
						}
					}
					fmt.sbprintf(b, "\t\t\tparser_begin(p, .%s, top.node)\n", nonterminal_start)
					fmt.sbprint(b, "\t\t\treturn .Continue\n")
				}
			}
			fmt.sbprint(b, "\t\t}")
		}

		// ε ベースケースがあれば else として出力
		has_epsilon := false
		for base_idx in loop.base_prods {
			base_prod := &rule.productions[base_idx]
			if len(base_prod.symbols) == 0 {
				has_epsilon = true
				fmt.sbprint(b, " else {\n")
				fmt.sbprintf(b, "\t\t\tparser_set_state(p, .%s)\n", op_state)
				fmt.sbprint(b, "\t\t\treturn .Continue\n")
				fmt.sbprint(b, "\t\t}")
				break
			}
		}

		if !has_epsilon && !first_if {
			fmt.sbprint(b, " else {\n")
			fmt.sbprintf(b, "\t\t\tparser_error(p, fmt.tprintf(\"Unexpected token in %s: %%v\", tk.type))\n", rule.name)
			fmt.sbprint(b, "\t\t\treturn .Break\n")
			fmt.sbprint(b, "\t\t}")
		}
		fmt.sbprint(b, "\n")
	}
}

// ========================================================================
// Token 定義ファイル (_token.odin) のコード生成
// ========================================================================

// Token 定義コード生成メイン
codegen_token :: proc(g: ^Grammar) -> string {
	b: strings.Builder
	strings.builder_init(&b)

	emit_token_header(&b, g)
	emit_token_type_enum(&b, g)
	emit_token_struct(&b, g)
	emit_token_functions(&b, g)

	return strings.to_string(b)
}

// Token ファイルのヘッダ (package 宣言 + 自動生成コメント)
@(private = "file")
emit_token_header :: proc(b: ^strings.Builder, g: ^Grammar) {
	pkg := g.package_name if len(g.package_name) > 0 else "parser"

	fmt.sbprintf(b,
`// Code generated by llpgen. DO NOT EDIT.
package %s

`, pkg)
}

// Token_Type enum の生成
@(private = "file")
emit_token_type_enum :: proc(b: ^strings.Builder, g: ^Grammar) {
	tk_enum := get_token_enum_type(g)

	fmt.sbprintf(b, "// トークン種別\n%s :: enum {{\n", tk_enum)
	for tok in g.tokens {
		fmt.sbprintf(b, "\t%s,\n", tok)
	}
	fmt.sbprint(b, "}\n\n")
}

// Token struct の生成 (Pos 構造体を含む)
@(private = "file")
emit_token_struct :: proc(b: ^strings.Builder, g: ^Grammar) {
	tk_type := get_token_type(g)
	tk_enum := get_token_enum_type(g)

	fmt.sbprintf(b,
`// 位置情報
Pos :: struct {{
	offset: int,
	line:   int,
	column: int,
}}

// トークン
%s :: struct {{
	type:     %s,
	consumed: bool,
	lexeme:   string,
	using pos: Pos,
}}

`, tk_type, tk_enum)
}

// Token 関連関数の生成 (consumed, is_term, consume_term)
@(private = "file")
emit_token_functions :: proc(b: ^strings.Builder, g: ^Grammar) {
	tk_type := get_token_type(g)
	tk_enum := get_token_enum_type(g)

	// consumed
	fmt.sbprintf(b,
`// トークンが期待通りか確認して消費
consumed :: proc(actual: ^%s, expected: %s) -> bool {{
	if actual.type == expected {{
		actual.consumed = true
		return true
	}}
	return false
}}

`, tk_type, tk_enum)

	// is_term / consume_term: term_tokens が定義されている場合のみ生成
	if len(g.term_tokens) > 0 {
		fmt.sbprintf(b, "// 文区切りトークンかチェック\nis_term :: proc(tk: ^%s) -> bool {{\n\treturn ", tk_type)
		for tok, i in g.term_tokens {
			if i > 0 {
				fmt.sbprint(b, " || ")
			}
			fmt.sbprintf(b, "tk.type == .%s", tok)
		}
		fmt.sbprint(b, "\n}\n\n")

		fmt.sbprintf(b,
`// 文区切りトークンを消費
consume_term :: proc(tk: ^%s) -> bool {{
	if is_term(tk) {{
		tk.consumed = true
		return true
	}}
	return false
}}

`, tk_type)
	}
}
